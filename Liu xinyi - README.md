# tf-model-segment

本周以mnist数据为样本，从实现普通DNN，简单CNN（2个卷积层，两个池化层，2个全连接层），到实现AlexNet。3种不同网络的accuracy在每个py文件里最后有注释出来。

遇到的问题：
1. 在搭一个神经网络的时候，为什么第一个中间层的激活函数一般用tanh？最后一层是softmax？每一层都应该用什么激活函数？本来用softmax但发现梯度下降没有改变accuracy
2. 什么时候用dropput？是否每个全连接层都是尽量用？概率多大比较合适？
3. 不知道alexnet是怎么改进出来的，是一点一点试出来的吗？
4. 电脑没有gpu，不知道怎么拆成2个/多个GPU上运行
5. Alexnet每次迭代都没有改变精度，不知道是optimizer的问题还是其他问题，暂时没找到
